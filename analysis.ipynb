{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac813e9-7789-4b06-b7db-6e85b1c64c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics, calibration\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import statsmodels.api as sm\n",
    "from pycaleva import CalibrationEvaluator # fork of https://martinweigl.github.io/pycaleva/ at https://github.com/KonKob/pycaleva/\n",
    "from nri_f import nri\n",
    "from confidenceinterval import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from ESCAPE_plotting import adjust_color_brightness, plot_dca\n",
    "from ESCAPE_evaluation import nagelkerke_coxsnell, calculate_dca, categorical_nri, bootstrap_nri, calculate_nri_with_threshold, smooth_array, bootstrap_ci, calculate_discrimination_slope\n",
    "from ESCAPE_stats import pairwise_comparison, chi2_or_fisher_test, mwu_or_ttest, build_flat_summary_dict, comparisons_versus_reference, build_flat_summary_reference_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58952d41-b8a3-4bed-b8c4-ef2062eb9383",
   "metadata": {},
   "source": [
    "## Loading and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c39b91-9905-4e6c-9b90-55698851ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access file\n",
    "data_file = \"data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241c284-4d5b-4b1a-8093-658a88324cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load table and transform\n",
    "df_data = pd.read_excel(data_file, header=None, index_col=0)\n",
    "df_data = df_data.T\n",
    "# remove empty entries\n",
    "df_data = df_data.loc[df_data[\"pseudonym\"].notna(), :]\n",
    "# replace 'error: missing input'\n",
    "df_data = df_data.replace({'error: missing input': np.nan})\n",
    "df_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fd7c6-94b2-4ae8-8239-b005b84d304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df_data.loc[df_data.duplicated(subset=[\"pseudonym\"]), \"pseudonym\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950d430-4e29-4000-b47e-6c49dc45c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pseudonyms to remove:\", \", \".join(df_data.loc[df_data[\"exclusion due to exclusion criteria\"]==\"yes\", \"pseudonym\"].astype(str).values))\n",
    "df_data = df_data.loc[df_data[\"exclusion due to exclusion criteria\"]!=\"yes\", :]\n",
    "print(\"pseudonyms with unfinished entry:\", \", \".join(df_data.loc[df_data[\"entry done\"]==\"no\", \"pseudonym\"].astype(str).values))\n",
    "df_data = df_data.loc[df_data[\"entry done\"]!=\"no\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1bea8-3851-401c-8104-0c0105b6843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = df_data[\"cohort\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92333c3d-9369-4fa8-b895-50876f65ccc5",
   "metadata": {},
   "source": [
    "## Definitions relevant for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcdef5-cb58-4cc0-888c-aa8bd2568e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_hex = {\"CardioExplorer\": \"#1F18A2\", 'CardioExplorer individual PTP': \"#1F18A2\", \"ESC 2024 PTP (formula)\": \"#AF1329\", \"ESC 2024 RF-CL (formula)\": \"#8C0D1F\", \"ESC 2019 PTP\": '#C21830', \"ESC 2013 PTP\": \"#F28C8C\", \"ESC 2024 CACS-CL (formula)\": \"#5A0A16\", \"ACC 2021 PTP\": \"#00294C\", \"ACC 2012 PTP\": \"#003366\", 'Diamond-Forrester': \"#08F767\", \"ESC 2024 RF-CL (formula with default 0 for number of risk factors and symptom score)\": \"#8C0D1F\", \"treat all\": \"#CECECE\", \"treat none\": \"#000000\", \"PROMISE Minimal-Risk Score\": \"#4c65a5\", \"CAD consortium score 2012 Basic PTP\": \"#d9dfee\", \"CAD consortium score 2012 Clinical PTP\": \"#d3d6e9\", \"clinical assessment of CAD pretest probability\": \"#FF9900\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e00b1b-d651-4347-93e7-9c0cd750e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_of_interest = ['previous CAD diagnostic', 'CAD previously known', 'length of stay in ED (for ED cohort only)', 'performed, planned, or recommended initial diagnostics', 'inpatient admission', 'clinical assessment of CAD pretest probability', 'diagnosis at discharge or inpatient admission', 'age [years]', 'sex', 'height [cm]', 'weight [kg]', 'presence and type of chest pain (CE)', 'presence and type of chest pain (ESC 2013/ACC 2012)', 'thoracic pain (ESC 2019)', 'thoracic pain (ACC 2021)', 'nicotine consumption (CE)', 'nicotine consumption (ESC 2024)', 'family history', 'dyslipidemia', 'hypertension', 'diabetes', 'diabetes medication', 'cholesterol lowering medication', 'tc aggregation inhibitor', 'RAS inhibitor', 'ca antagonist', 'betablocker', 'diuretic', 'organic nitrate', 'systolic blood pressure [mmHg]', 'diastolic blood pressure [mmHg]', 'pathological Q-waves at ECG', 'pancreatic amylase [U/l]', 'alkaline phosphatase [U/l]', 'hs troponin T [pg/ml]', 'alanin-aminotransferase [U/l]', 'glucose [mg/dl]', 'bilirubin (total) [mg/dl]', 'urea [mg/dl]', 'uric acid [mg/dl]', 'cholesterol (total) [mg/dl]', 'high-density lipoprotein cholesterol [mg/dl]', 'low-density lipoprotein cholesterol [mg/dl]', 'protein (total) [g/l]', 'albumin [g/l]', 'leucocytes [*1000/yl]', 'mean corpuscular haemoglobin concentration [g/dl]', 'coronary artery calcification score from previous chest CT scans', 'CE (primary care calibration) score', 'CE (primary care calibration) PTP', 'Diamond-Forrester', 'ACC 2012 PTP', 'ESC 2013 PTP', 'ESC 2019 PTP', 'ACC 2021 PTP', 'PROMISE Minimal-Risk Score', 'ESC 2024 RF-CL (table)', 'ESC 2024 PTP (formula)', 'ESC 2024 RF-CL (formula)', 'ESC 2024 CACS-CL (formula)', \"diabetes medication\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5f067-5c20-4fed-bd77-4d5ac89334d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome definitions\n",
    "df_data[\"atrial fibrillation dummy outcome\"] = df_data[\"atrial fibrillation\"].replace({\"yes\": 1, \"no\": 0})\n",
    "df_data[\"CAD progression or new CAD\"] = df_data[\"new CAD or CAD progression 3 months\"].replace({\"inconclusive\": np.nan, \"excluded\": 0, \"diagnosed\": 1})\n",
    "df_data[\"known CAD or new CAD\"] = ((df_data[\"new CAD or CAD progression 3 months\"]==\"diagnosed\") | (df_data[\"CAD previously known\"] == \"yes\")).astype(int)\n",
    "\n",
    "outcomes = [\"atrial fibrillation dummy outcome\", \"CAD progression or new CAD\", \"known CAD or new CAD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205a774-d81f-47c1-98c7-d4163adca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_compare = [\n",
    "    [\"CardioExplorer individual PTP\", \"ESC 2024 PTP (formula)\", \"ESC 2024 RF-CL (formula with default 0 for number of risk factors and symptom score)\", \"clinical assessment of CAD pretest probability\"],\n",
    "    [\"CardioExplorer\", \"ESC 2024 PTP (formula)\", \"ESC 2024 RF-CL (formula with default 0 for number of risk factors and symptom score)\", \"clinical assessment of CAD pretest probability\"],\n",
    "    [\"CardioExplorer individual PTP\", \"ESC 2024 PTP (formula)\", \"ESC 2024 RF-CL (formula)\", \"clinical assessment of CAD pretest probability\"],\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6650f2e-c905-4062-b7c5-07de847a5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudonyms_per_group = {\n",
    "    \"total\": df_data[\"pseudonym\"].unique(), \n",
    "    \"ED\": df_data.loc[df_data[\"cohort\"]==\"ED\", \"pseudonym\"].unique(),\n",
    "    \"DZHI\": df_data.loc[df_data[\"cohort\"]==\"DZHI\", \"pseudonym\"].unique(),\n",
    "    \"ED_CCS\": df_data.loc[(df_data[\"cohort\"]==\"ED\") & (df_data[\"inclusion criterium\"]==\"CCS\"), \"pseudonym\"].unique(),\n",
    "    \"ED_AF\": df_data.loc[(df_data[\"cohort\"]==\"ED\") & (df_data[\"inclusion criterium\"]==\"AF\"), \"pseudonym\"].unique(),\n",
    "    \"DZHI_HCM\": df_data.loc[(df_data[\"cohort\"]==\"DZHI\") & (df_data[\"inclusion criterium\"]==\"HCM\"), \"pseudonym\"].unique(),\n",
    "    \"DZHI_noHCM\": df_data.loc[(df_data[\"cohort\"]==\"DZHI\") & (df_data[\"inclusion criterium\"]!=\"HCM\"), \"pseudonym\"].unique(),\n",
    "    \"DZHI_nopriorCAD\": df_data.loc[(df_data[\"cohort\"]==\"DZHI\") & (df_data[\"CAD previously known\"]==\"no\"), \"pseudonym\"].unique(),\n",
    "    \"DZHI_priorCAD\": df_data.loc[(df_data[\"cohort\"]==\"DZHI\") & (df_data[\"CAD previously known\"]==\"yes\"), \"pseudonym\"].unique(),\n",
    "    \"allcohorts_nopriorCAD\": df_data.loc[(df_data[\"CAD previously known\"]==\"no\"), \"pseudonym\"].unique(),\n",
    "    \"allcohorts_priorCAD\": df_data.loc[(df_data[\"CAD previously known\"]==\"yes\"), \"pseudonym\"].unique(),\n",
    "    \"male\": df_data.loc[(df_data[\"sex\"]==\"male\"), \"pseudonym\"].unique(),\n",
    "    \"female\": df_data.loc[(df_data[\"sex\"]==\"female\"), \"pseudonym\"].unique(),\n",
    "    \"no_stdtroponin_ED\": df_data.loc[(df_data[\"hs troponin T [pg/ml]\"].notna()) & (df_data[\"cohort\"]==\"ED\"), \"pseudonym\"].unique(),\n",
    "    \"stdtroponin_ED\": df_data.loc[(~df_data[\"hs troponin T [pg/ml]\"].notna()) & (df_data[\"cohort\"]==\"ED\"), \"pseudonym\"].unique(),\n",
    "    \"furtherCADdiagnostic\": df_data.loc[df_data[\"CAD progression or new CAD\"].notna(), \"pseudonym\"].unique(),\n",
    "    \"no_furtherCADdiagnostic\": df_data.loc[~df_data[\"CAD progression or new CAD\"].notna(), \"pseudonym\"].unique(),\n",
    "    \"primary_diagnosed\": df_data.loc[df_data[\"CAD progression or new CAD\"]==1, \"pseudonym\"].unique(),\n",
    "    \"primary_excluded\": df_data.loc[df_data[\"CAD progression or new CAD\"]==0, \"pseudonym\"].unique(),\n",
    "                        }\n",
    "\n",
    "per_group_variables_scale = {\n",
    "    \n",
    "    \"diabetes\": \"categorial\",\n",
    "    \"diabetes medication\": \"categorial\",\n",
    "    \"dyslipidemia\": \"categorial\",\n",
    "    \"dyspnea\": \"categorial\",\n",
    "    \"family history\": \"categorial\",\n",
    "    \"female sex\": \"categorial\",\n",
    "    \"hypertension\": \"categorial\",\n",
    "    \"inpatient admission\": \"categorial\",\n",
    "    \"pathological Q-waves at ECG\": \"categorial\",\n",
    "    \"atrial fibrillation\": \"categorial\",\n",
    "    \"CAD previously known\": \"categorial\",\n",
    "\n",
    "    \"presence and type of chest pain (ESC 2013/ACC 2012)\": \"categorial\",\n",
    "    \"nicotine consumption (CE)\": \"categorial\",\n",
    "    \"clinical assessment of CAD pretest probability\": \"categorial\",\n",
    "    \"pathological Q-waves at ECG\": \"categorial\", \n",
    "    \"inpatient admission\": \"categorial\", \n",
    "    \"cholesterol lowering medication\": \"categorial\", \n",
    "    \"tc aggregation inhibitor\": \"categorial\", \n",
    "    \"RAS inhibitor\": \"categorial\", \n",
    "    \"ca antagonist\": \"categorial\", \n",
    "    \"betablocker\": \"categorial\", \n",
    "    \"diuretic\": \"categorial\", \n",
    "    \"organic nitrate\": \"categorial\",\n",
    "\n",
    "    \"age [years]\": \"rational\",\n",
    "    \"height [cm]\": \"rational\",\n",
    "    \"weight [kg]\": \"rational\",\n",
    "    \"coronary artery calcification score from previous chest CT scans\": \"rational\",\n",
    "    \"systolic blood pressure [mmHg]\": \"rational\",\n",
    "    \"diastolic blood pressure [mmHg]\": \"rational\",\n",
    "    \"BMI [kg/m^2]\": \"rational\", \n",
    "    \"pancreatic amylase [U/l]\": \"rational\", \n",
    "    \"alkaline phosphatase [U/l]\": \"rational\", \n",
    "    \"hs troponin T [pg/ml]\": \"rational\",\n",
    "    \"alanin-aminotransferase [U/l]\": \"rational\", \n",
    "    \"glucose [mg/dl]\": \"rational\", \n",
    "    \"bilirubin (total) [mg/dl]\": \"rational\", \n",
    "    \"urea [mg/dl]\": \"rational\", \n",
    "    \"uric acid [mg/dl]\": \"rational\", \n",
    "    \"cholesterol (total) [mg/dl]\": \"rational\", \n",
    "    \"high-density lipoprotein cholesterol [mg/dl]\": \"rational\", \n",
    "    \"low-density lipoprotein cholesterol [mg/dl]\": \"rational\", \n",
    "    \"protein (total) [g/l]\": \"rational\", \n",
    "    \"albumin [g/l]\": \"rational\", \n",
    "    \"leucocytes [*1000/yl]\": \"rational\", \n",
    "    \"mean corpuscular haemoglobin concentration [g/dl]\": \"rational\",\n",
    "\n",
    "    \"previous CAD diagnostic\": \"free text\",\n",
    "    \"performed, planned, or recommended initial diagnostics\": \"free text\",\n",
    "    \"diagnosis at discharge or inpatient admission\": \"free text\",\n",
    "    \"diagnosis at discharge or inpatient admission\": \"free text\",\n",
    "                                \n",
    "                           }\n",
    "for outcome in outcomes:\n",
    "    per_group_variables_scale[outcome] = \"categorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906092f-2f04-4a54-a90a-18a6f240fb3e",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86603d8-cd9d-487f-917b-f40bb12b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan_replaced = df_data.replace(\"error: missing input\", np.nan)\n",
    "df_number_of_missing_values = df_nan_replaced.isna().sum()[missing_values_of_interest]\n",
    "df_missing_values = df_number_of_missing_values.loc[df_number_of_missing_values>0]\n",
    "df_missing_values_percent = df_missing_values/df_data.shape[0] \n",
    "df_values_present_percent = 1 - df_missing_values_percent\n",
    "missing_parameter_categories = [(\"anamnesis\", ['presence and type of chest pain (ESC 2013/ACC 2012)', 'thoracic pain (ESC 2019)', 'thoracic pain (ACC 2021)', 'nicotine consumption (CE)', 'nicotine consumption (ESC 2024)',\n",
    "       'family history', 'dyslipidemia', 'hypertension', 'diabetes', 'cholesterol lowering medication', 'tc aggregation inhibitor', 'RAS inhibitor', 'ca antagonist', 'betablocker', 'diuretic', 'organic nitrate', \"diabetes medication\"]),\n",
    "                     (\"laboratory results\", ['pancreatic amylase [U/l]', 'hs troponin T [pg/ml]', 'bilirubin (total) [mg/dl]', 'cholesterol (total) [mg/dl]', 'high-density lipoprotein cholesterol [mg/dl]',\n",
    "       'low-density lipoprotein cholesterol [mg/dl]', 'albumin [g/l]', 'alkaline phosphatase [U/l]', 'alanin-aminotransferase [U/l]', 'uric acid [mg/dl]', 'protein (total) [g/l]']),\n",
    "                    (\"test results\", ['coronary artery calcification score from previous chest CT scans', 'systolic blood pressure [mmHg]', 'diastolic blood pressure [mmHg]', 'pathological Q-waves at ECG', ]),\n",
    "                    (\"scores\", ['CE (primary care calibration) score', 'Diamond-Forrester', 'ACC 2012 PTP', 'ESC 2013 PTP', 'ESC 2019 PTP', 'ACC 2021 PTP', 'PROMISE Minimal-Risk Score', 'ESC 2024 RF-CL (table)',\n",
    "       'ESC 2024 RF-CL (formula)', 'ESC 2024 CACS-CL (formula)']),\n",
    "                    (\"data extraction\", ['diagnosis at discharge or inpatient admission', 'previous CAD diagnostic']) ,                         \n",
    "                    (\"outcomes\", outcomes)\n",
    "                               ]\n",
    "missing_parameter_categories_dict = dict(missing_parameter_categories)\n",
    "defined_missing_parameters = [p for _, params in missing_parameter_categories for p in params]\n",
    "undefined_missing_parameters = [p for p in df_missing_values.index if p not in defined_missing_parameters]\n",
    "if undefined_missing_parameters:\n",
    "    print(\"The following parameters are missing but do not belong to a category!\", undefined_missing_parameters)\n",
    "print(\"The following parameters are missing for some entries.\")\n",
    "for category, params in missing_parameter_categories:\n",
    "    print(category)\n",
    "    params_really_missing = [p for p in params if p in df_missing_values.index]\n",
    "    for p in params_really_missing:\n",
    "        print(\"  - \", p, round(df_missing_values_percent[p], 2))\n",
    "\n",
    "missing_parameter_categories_dict[\"laboratory results\"].remove(\"hs troponin T [pg/ml]\")\n",
    "entries_with_missing_laboratory_results_except_trop = df_data.loc[(df_data[missing_parameter_categories_dict[\"laboratory results\"]].isna().any(axis=1)), [\"pseudonym\", \"date of inclusion\", \"inclusion criterium\", \"cohort\"] + missing_parameter_categories_dict[\"laboratory results\"]]\n",
    "print(\"\\n\\nLaboratory results are missing in the following cohorts, inclusion criteria and subjects:\")\n",
    "for cohort in cohorts:\n",
    "    print(\"\\n\", cohort)\n",
    "    cohort_entries_with_missing_lab_results = entries_with_missing_laboratory_results_except_trop.loc[entries_with_missing_laboratory_results_except_trop[\"cohort\"]==cohort]\n",
    "    for inclusion_criterium in cohort_entries_with_missing_lab_results[\"inclusion criterium\"].unique():\n",
    "        print(\"  \", inclusion_criterium)\n",
    "        print(cohort_entries_with_missing_lab_results.loc[cohort_entries_with_missing_lab_results[\"inclusion criterium\"]==inclusion_criterium, missing_parameter_categories_dict[\"laboratory results\"]].isna().sum(), cohort_entries_with_missing_lab_results.loc[cohort_entries_with_missing_lab_results[\"inclusion criterium\"]==inclusion_criterium, \"pseudonym\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee143627-b0fa-492c-a498-89b6099b5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"hs troponin T [pg/ml]\"] = df_data[\"hs troponin T [pg/ml]\"].replace({\"<3\": 0})\n",
    "df_data[\"alanin-aminotransferase [U/l]\"] = df_data[\"alanin-aminotransferase [U/l]\"].replace({\"<5\": 0})\n",
    "df_data[\"bilirubin (total) [mg/dl]\"] = df_data[\"bilirubin (total) [mg/dl]\"].replace({\"<0.2\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cafdd-f82d-40dc-b3dd-d8170b7215fd",
   "metadata": {},
   "source": [
    "## variables per cohort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3bc3d3-bafe-4b66-b98b-6c65f4b952fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df_list_for_binaries_or_continous = []\n",
    "group_df_list_for_categorial = []\n",
    "group_variable_comparison = pd.DataFrame({}, columns=[\"variable\", \"mean\", \"std\", \"group\", \"n\", \"n information available\", \"group n\"])\n",
    "\n",
    "for group, pseudonyms in pseudonyms_per_group.items():\n",
    "    binary_variables = [\"CAD previously known\", \"family history\", \"dyslipidemia\", \"hypertension\", \"diabetes\", \"atrial fibrillation\", \"pathological Q-waves at ECG\", \"inpatient admission\", \"cholesterol lowering medication\", \"tc aggregation inhibitor\", \"RAS inhibitor\", \"ca antagonist\", \"betablocker\", \"diuretic\", \"organic nitrate\"]\n",
    "    for outcome in outcomes:\n",
    "        binary_variables.append(outcome)\n",
    "    numerical_variables = [\"age [years]\", \"height [cm]\", \"weight [kg]\", \"BMI [kg/m^2]\", \"systolic blood pressure [mmHg]\", \"diastolic blood pressure [mmHg]\", \"coronary artery calcification score from previous chest CT scans\", \"pancreatic amylase [U/l]\", \"alkaline phosphatase [U/l]\", \"hs troponin T [pg/ml]\", \"alanin-aminotransferase [U/l]\", \"glucose [mg/dl]\", \"bilirubin (total) [mg/dl]\", \"urea [mg/dl]\", \"uric acid [mg/dl]\", \"cholesterol (total) [mg/dl]\", \"high-density lipoprotein cholesterol [mg/dl]\", \"low-density lipoprotein cholesterol [mg/dl]\", \"protein (total) [g/l]\", \"albumin [g/l]\", \"leucocytes [*1000/yl]\", \"mean corpuscular haemoglobin concentration [g/dl]\"]\n",
    "    group_df = df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms), :]\n",
    "    # binary variables\n",
    "    group_df[\"female sex\"] = group_df[\"sex\"]==\"female\"\n",
    "    group_df[\"dyspnea\"] = group_df[\"dyspnoea as shortness of breath and/or trouble catching breath aggravated by physical exertion (ESC 2024)\"]==\"yes\"\n",
    "    for v in binary_variables:\n",
    "        group_df[v] = group_df[v].replace({\"yes\": 1, \"M61\": 1, \"discharge against medical advice\": 1, \"no\": 0}).astype(float)\n",
    "    group_df[\"group\"] = group\n",
    "    missing_values = {}\n",
    "    for v in numerical_variables + binary_variables + [\"sex\", \"dyspnoea as shortness of breath and/or trouble catching breath aggravated by physical exertion (ESC 2024)\", \"nicotine consumption (CE)\", \"clinical assessment of CAD pretest probability\", \"diabetes medication\"]:\n",
    "        if v==\"dyspnoea as shortness of breath and/or trouble catching breath aggravated by physical exertion (ESC 2024)\":\n",
    "            key = \"dyspnea\"\n",
    "        elif v==\"sex\":\n",
    "            key = \"female sex\"\n",
    "        else:\n",
    "            key = v\n",
    "        missing_values[key] = group_df.loc[group_df[key].isna(), :].shape[0] \n",
    "        \n",
    "    chest_pain = group_df[\"presence and type of chest pain (ESC 2013/ACC 2012)\"].value_counts()\n",
    "    chest_pain[\"none\"] = group_df.loc[group_df[\"presence and type of chest pain (ESC 2013/ACC 2012)\"].isna(), :].shape[0]\n",
    "    missing_values[\"chest pain\"] = 0\n",
    "    nicotine_consumption = group_df[\"nicotine consumption (CE)\"].value_counts()\n",
    "    clinical_cad_ptp = group_df[\"clinical assessment of CAD pretest probability\"].value_counts()\n",
    "    diabetes_medication = group_df[\"diabetes medication\"].value_counts()\n",
    "\n",
    "    group_df_list_for_binaries_or_continous.append(group_df[numerical_variables + binary_variables + [\"female sex\", \"dyspnea\", \"group\"]])\n",
    "    group_df_list_for_categorial.append(group_df[[\"presence and type of chest pain (ESC 2013/ACC 2012)\", \"nicotine consumption (CE)\", \"clinical assessment of CAD pretest probability\", \"diabetes medication\", \"group\"]])\n",
    "    \n",
    "    n = group_df.shape[0]\n",
    "    means = group_df.loc[:, numerical_variables + binary_variables + [\"female sex\", \"dyspnea\"]].mean(numeric_only=True) # continuous/binary categories\n",
    "    stds = group_df.loc[:, numerical_variables].std(numeric_only=True) # continuous categories\n",
    "    positive_values = group_df.loc[:, binary_variables + [\"female sex\", \"dyspnea\"]].sum(numeric_only=True)\n",
    "    for v in means.index:\n",
    "        group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[v, round(means[v], 2), round(stds[v], 2) if v in stds else np.nan, group, int(positive_values[v]) if v not in stds else np.nan, n-missing_values[v], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "    for v in df_data[\"nicotine consumption (CE)\"].unique():\n",
    "        if not pd.isna(v):\n",
    "            if v in nicotine_consumption:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"nicotine consumption (CE)\", v), np.nan, np.nan, group, nicotine_consumption[v], n-missing_values[\"nicotine consumption (CE)\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "            else:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"nicotine consumption (CE)\", v), np.nan, np.nan, group, 0, n-missing_values[\"nicotine consumption (CE)\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "\n",
    "    for v in df_data[\"diabetes medication\"].unique():\n",
    "        if not pd.isna(v):\n",
    "            if v in diabetes_medication:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"diabetes medication\", v), np.nan, np.nan, group, diabetes_medication[v], n-missing_values[\"diabetes medication\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "            else:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"diabetes medication\", v), np.nan, np.nan, group, 0, n-missing_values[\"diabetes medication\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "                \n",
    "    for v in df_data[\"presence and type of chest pain (ESC 2013/ACC 2012)\"].unique():\n",
    "        if not pd.isna(v):\n",
    "            if v in chest_pain:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"presence and type of chest pain (ESC 2013/ACC 2012)\", v), np.nan, np.nan, group, chest_pain[v], n-missing_values[\"chest pain\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "            else:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"presence and type of chest pain (ESC 2013/ACC 2012)\", v), np.nan, np.nan, group, 0, n-missing_values[\"chest pain\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "    group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"presence and type of chest pain (ESC 2013/ACC 2012)\",  \"none\"), np.nan, np.nan, group, chest_pain[\"none\"], n-missing_values[\"chest pain\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "    for v in df_data[\"clinical assessment of CAD pretest probability\"].unique():\n",
    "        if not pd.isna(v):\n",
    "            if v in clinical_cad_ptp:\n",
    "                    group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"clinical assessment of CAD pretest probability\", v), np.nan, np.nan, group, clinical_cad_ptp[v], n-missing_values[\"clinical assessment of CAD pretest probability\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "            else:\n",
    "                group_variable_comparison = pd.concat([group_variable_comparison, pd.DataFrame([[(\"clinical assessment of CAD pretest probability\", v), np.nan, np.nan, group, 0, n-missing_values[\"clinical assessment of CAD pretest probability\"], n]], columns=group_variable_comparison.columns)], ignore_index=True)\n",
    "\n",
    "pd.options.mode.chained_assignment = \"warn\"\n",
    "\n",
    "group_comparison_subject_binaries_or_continous = pd.concat(group_df_list_for_binaries_or_continous, ignore_index=True).replace({False: 0, True: 1})\n",
    "group_comparison_subject_categorial = pd.concat(group_df_list_for_categorial, ignore_index=True)\n",
    "group_comparison_subject_categorial[\"presence and type of chest pain (ESC 2013/ACC 2012)\"]=group_comparison_subject_categorial[\"presence and type of chest pain (ESC 2013/ACC 2012)\"].replace(np.nan, \"None\")\n",
    "\n",
    "group_comparison_per_subject = pd.merge(group_comparison_subject_categorial, group_comparison_subject_binaries_or_continous, left_index=True, right_index=True)\n",
    "group_comparison_per_subject = group_comparison_per_subject.rename(columns={\"group_x\": \"group\"})\n",
    "group_comparison_per_subject = group_comparison_per_subject.drop(\"group_y\", axis=1)\n",
    "\n",
    "per_group_variables_statistics = pairwise_comparison(group_comparison_per_subject, per_group_variables_scale, group_pairs = [(\"ED\", \"DZHI\"), (\"ED_CCS\", \"ED_AF\"), (\"DZHI_HCM\", \"DZHI_noHCM\"), (\"DZHI_priorCAD\", \"DZHI_nopriorCAD\"), (\"allcohorts_nopriorCAD\", \"allcohorts_priorCAD\"), (\"male\", \"female\"), (\"no_stdtroponin_noHCM\", \"stdtroponin_noHCM\"), (\"no_stdtroponin_ED\", \"stdtroponin_ED\")])\n",
    "flat_summary_dict = build_flat_summary_dict(group_variable_comparison, per_group_variables_statistics, per_group_variables_scale)\n",
    "per_group_variables = pd.DataFrame.from_dict(flat_summary_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fec3b8-b771-4bb4-8aac-fe5133266165",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_and_continuous_reference = pd.read_excel(\"e.xlsx\", index_col=0, sheet_name=\"binary_and_continuous_variables\")\n",
    "binary_and_continuous_reference[\"mean_standard_unit\"] = binary_and_continuous_reference[\"mean_standard_unit\"].astype(float)\n",
    "categorial_reference = pd.read_excel(\"e.xlsx\", index_col=[0, 1], sheet_name=\"categorial_variables\")\n",
    "reference = {\"binary_and_continuous\": binary_and_continuous_reference, \"categorial\": categorial_reference}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838ff82-99b5-4b2c-ba0b-1ca67366876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_against_reference = comparisons_versus_reference(df_data, reference, pseudonyms_per_group, total_n=696, name = \"e\")\n",
    "summary_reference_dict = build_flat_summary_reference_dict(variables_against_reference, name = \"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a356be6-b264-4ed1-ac09-a5ef232e96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "against_reference_variables = pd.DataFrame.from_dict(summary_reference_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260c535-f048-4164-830c-9058da088597",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_and_continuous_reference_z = pd.read_excel(\"z.xlsx\", index_col=0, sheet_name=\"binary_and_continuous_variables\")\n",
    "binary_and_continuous_reference_z[\"mean_standard_unit\"] = binary_and_continuous_reference_z[\"mean_standard_unit\"].astype(float)\n",
    "categorial_reference_z = pd.read_excel(\"z.xlsx\", index_col=[0, 1], sheet_name=\"categorial_variables\")\n",
    "reference_z = {\"binary_and_continuous\": binary_and_continuous_reference_z, \"categorial\": categorial_reference_z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf46f0-d0bf-4c15-a8c9-cba8f47e9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_against_reference_z = comparisons_versus_reference(df_data, reference_z, pseudonyms_per_group, total_n=987, name=\"z\")\n",
    "summary_reference_dict_z = build_flat_summary_reference_dict(variables_against_reference_z, name = \"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7173ef-76a6-4658-a7a1-4a1ccb73cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "against_reference_variables_z = pd.DataFrame.from_dict(summary_reference_dict_z, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fe620-8c23-4343-8e25-2faaf5ceda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time in ED per group\")\n",
    "for group in [\"ED\", \"ED_CCS\", \"ED_AF\"]: #\"ED_inpatient\", \"ED_no_inpatient\", \"ED_inpatient_CCS\", \"ED_no_inpatient_CCS\", \"ED_inpatient_AF\", \"ED_no_inpatient_AF\"\n",
    "    print(group, round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).mean(), 2), \"+-\", round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).std(), 2), \"mean +- std\")\n",
    "    print(group, round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).median(), 2), \"(\", round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).quantile(0.25), 2), \"-\", round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).quantile(0.75), 2), \")\", \"median (25-75% quantile range)\")\n",
    "    print(group, \"min:\", round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).min(), 2), \"max:\", round(df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[group]), \"length of stay in ED (for ED cohort only)\"].apply(lambda x: x.total_seconds()/3600).max(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71879b4-ed25-4d9f-84c1-71f15d230898",
   "metadata": {},
   "source": [
    "## slice for a specific cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861aad5-3a50-41ea-b651-403305c82bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice for specific cohort\n",
    "cohort_slice = \"total\"    # ED, DZHI, male, female, HCM, DZHI_HCM, DZHI_noHCM, ED_CCS, stdtroponin, no_stdtroponin\n",
    "df_data = df_data.loc[df_data[\"pseudonym\"].isin(pseudonyms_per_group[cohort_slice]), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9361ba-2ba5-412d-9cd5-a2260aa5cffe",
   "metadata": {},
   "source": [
    "## evaluation of calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6a7f9-dcad-4b24-aaf3-e7cf1c15820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_PTP_categories = {\"<5%\": 0.05, \"~20%\": 0.2, \"~50%\": 0.5, \">75%\": 0.75, \"error:missing input\": np.nan} \n",
    "clinical_PTP_categories = {\"very low\": 0.05, \"low\": 0.45, \"moderate\": 0.45, \"high\": 0.45, \"very high\": 0.9999}\n",
    "PROMISE_PTP_categories = lambda l: 0.05 if l > 0.34 else 0.9999\n",
    "df_scores = df_data.loc[:, [\"pseudonym\", \"CE (primary care calibration) score\", \"ESC 2024 PTP (formula)\", \"ESC 2024 RF-CL (formula)\", \"Diamond-Forrester\", \"ACC 2012 PTP\", \"ESC 2013 PTP\", \"ESC 2019 PTP\", \"ACC 2021 PTP\", \"ESC 2024 RF-CL (formula with default 0 for number of risk factors and symptom score)\", \"PROMISE Minimal-Risk Score\", \"CAD consortium score 2012 Basic PTP\", \"CAD consortium score 2012 Clinical PTP\", \"clinical assessment of CAD pretest probability\", \"CE individual PTP\"]]\n",
    "df_scores.loc[:, [\"Diamond-Forrester\", \"ACC 2012 PTP\", \"ESC 2013 PTP\", \"ESC 2019 PTP\", \"ACC 2021 PTP\"]] = df_scores.loc[:, [\"Diamond-Forrester\", \"ACC 2012 PTP\", \"ESC 2013 PTP\", \"ESC 2019 PTP\", \"ACC 2021 PTP\"]]/100\n",
    "df_scores_copy = df_scores.copy()\n",
    "df_scores.columns = pd.MultiIndex.from_product([df_scores.columns, ['score_values']])\n",
    "df_scores_copy.columns = pd.MultiIndex.from_product([df_scores_copy.columns, ['ptp_values']])\n",
    "df_scores = pd.concat([df_scores, df_scores_copy], axis=1)\n",
    "df_scores = df_scores.sort_index(axis=1, level=0)\n",
    "\n",
    "df_scores[(\"CE (primary care calibration) score\", \"ptp_values\")] = df_data.loc[:, \"CE (primary care calibration) PTP\"].replace(CE_PTP_categories)\n",
    "df_scores[(\"CE individual PTP\", \"score_values\")] =  df_data.loc[:, \"CE (primary care calibration) score\"]\n",
    "df_scores[(\"PROMISE Minimal-Risk Score\", \"ptp_values\")] = df_data.loc[:, \"PROMISE Minimal-Risk Score\"].apply(PROMISE_PTP_categories) # PMRS provides only a binary classification based on a threshold of 0.34\n",
    "df_scores[(\"clinical assessment of CAD pretest probability\", \"score_values\")] = df_scores[(\"clinical assessment of CAD pretest probability\", \"score_values\")].replace(clinical_PTP_categories)\n",
    "df_scores[(\"clinical assessment of CAD pretest probability\", \"ptp_values\")] = df_scores[(\"clinical assessment of CAD pretest probability\", \"score_values\")].replace(clinical_PTP_categories)\n",
    "df_scores = df_scores.drop((\"pseudonym\", \"ptp_values\"), axis=1)\n",
    "df_scores.columns = pd.MultiIndex.from_tuples([(\"pseudonym\", \" \") if col == (\"pseudonym\", \"score_values\") else col for col in df_scores.columns])\n",
    "df_scores.columns = pd.MultiIndex.from_tuples([(\"CardioExplorer\", \"score_values\") if col == (\"CE (primary care calibration) score\", \"score_values\") else col for col in df_scores.columns])\n",
    "df_scores.columns = pd.MultiIndex.from_tuples([(\"CardioExplorer\", \"ptp_values\") if col == (\"CE (primary care calibration) score\", \"ptp_values\") else col for col in df_scores.columns])\n",
    "\n",
    "df_scores.columns = pd.MultiIndex.from_tuples([(\"CardioExplorer individual PTP\", \"score_values\") if col == (\"CE individual PTP\", \"score_values\") else col for col in df_scores.columns])\n",
    "df_scores.columns = pd.MultiIndex.from_tuples([(\"CardioExplorer individual PTP\", \"ptp_values\") if col == (\"CE individual PTP\", \"ptp_values\") else col for col in df_scores.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a856f6a-e312-409e-bb9d-9842ed38dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_comparison_scores_and_outcomes = {}\n",
    "for score_comparison in scores_to_compare:\n",
    "    for score in score_comparison:\n",
    "        print(score, df_scores.loc[:, score].dropna().shape[0])\n",
    "    df_scores_for_comparison = df_scores.loc[:, [(score, v) for score in score_comparison for v in [\"ptp_values\", \"score_values\"]]+[(\"pseudonym\", \" \")]]\n",
    "    df_scores_for_comparison = df_scores_for_comparison.dropna()\n",
    "    df_scores_for_comparison = df_scores_for_comparison.convert_dtypes()\n",
    "    df_scores_for_comparison[\"pseudonym\"] = df_scores_for_comparison[\"pseudonym\"].astype(int)\n",
    "    score_comparison_scores_and_outcomes[\" vs. \".join(score_comparison)] = {\"score_comparison\": score_comparison, \"df_scores_for_comparison\": df_scores_for_comparison}\n",
    "    \n",
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    df_scores_for_comparison = score_comparison_scores_and_outcomes[score_comparison_key][\"df_scores_for_comparison\"]\n",
    "    score_comparison = score_comparison_scores_and_outcomes[score_comparison_key][\"score_comparison\"]\n",
    "    score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"] = {}\n",
    "    for outcome in outcomes:\n",
    "        subjects_with_available_score = df_scores_for_comparison.loc[:, (\"pseudonym\", \" \")].values\n",
    "        subjects_with_available_outcomes = df_data.loc[df_data.loc[:, outcome].notna(), \"pseudonym\"].values\n",
    "        subjects_with_available_score_and_outcome = [s for s in subjects_with_available_score if s in subjects_with_available_outcomes]\n",
    "        print(score_comparison_key, \"for\", outcome, \"\\nsubjects with scores available:\", len(subjects_with_available_score), \"\\nsubjects with outcome available:\", len(subjects_with_available_outcomes), \"\\nsubjects with score and outcome available:\", len(subjects_with_available_score_and_outcome))\n",
    "        y_true = df_data.loc[df_data.loc[:, \"pseudonym\"].isin(subjects_with_available_score_and_outcome), outcome].replace({\"yes\": 1, \"no\": 0}).astype(int).values\n",
    "        \n",
    "        scores = {}\n",
    "        for score in score_comparison:\n",
    "            scores[score] = {\"score_values\": df_scores_for_comparison.loc[df_scores_for_comparison.loc[:, (\"pseudonym\", \" \")].isin(subjects_with_available_score_and_outcome), (score, \"score_values\")].values, \"ptp_values\": df_scores_for_comparison.loc[df_scores_for_comparison.loc[:, (\"pseudonym\", \" \")].isin(subjects_with_available_score_and_outcome), (score, \"ptp_values\")].values}\n",
    "        score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"][outcome] = {\"y_true\": y_true, \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2a623-6945-4ee9-9376-c0d1e658f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    # evaluation\n",
    "    evaluation_per_score_and_outcome = {}\n",
    "    for outcome, outcome_dict in score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"].items():\n",
    "        y_true = outcome_dict[\"y_true\"]\n",
    "        scores = outcome_dict[\"scores\"]\n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            prevalence = np.mean(y_true)\n",
    "            n = len(y_true)\n",
    "            evaluation_per_score_and_outcome[outcome] = {\"extreme_strategies\": {}, \"scores\": {}}\n",
    "            dca_df_all = calculate_dca(y_true, np.ones(len(y_true)), model_name=\"treat all\", harm=0)\n",
    "            dca_df_none = calculate_dca(y_true, np.zeros(len(y_true)), model_name=\"treat none\", harm=0)\n",
    "            evaluation_per_score_and_outcome[outcome][\"extreme_strategies\"][\"treat all\"] = {\"dca_df\": dca_df_all}\n",
    "            evaluation_per_score_and_outcome[outcome][\"extreme_strategies\"][\"treat none\"] = {\"dca_df\": dca_df_none}\n",
    "            \n",
    "            for score, score_dict in scores.items(): \n",
    "                y_pred_prob = score_dict[\"ptp_values\"]\n",
    "                score_values = score_dict[\"score_values\"]\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score] = {}\n",
    "                \n",
    "                # overall performance\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"brier\"] = metrics.brier_score_loss(y_true, y_pred_prob)\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"brier ci\"] = bootstrap_ci(y_true, y_pred_prob, metrics.brier_score_loss)\n",
    "\n",
    "                cs_r2, nk_r2, _, _ = nagelkerke_coxsnell(y_true, y_pred_prob)\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"nk_r2\"] = nk_r2\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"cs_r2\"] = cs_r2\n",
    "                \n",
    "                # discrimination\n",
    "                auc, auc_ci = roc_auc_score(y_true, score_values, confidence_level=0.95)\n",
    "                discrimination_slope = calculate_discrimination_slope(y_true, score_values)\n",
    "                discrimination_slope_ci = bootstrap_ci(y_true, score_values, calculate_discrimination_slope)\n",
    "        \n",
    "                for threshold in [0.05, 0.15, 0.85]:\n",
    "                    y_pred_outcome = y_pred_prob > threshold\n",
    "                    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred_outcome).ravel()\n",
    "                    sensitivity = tp / (tp + fn)\n",
    "                    specificity = tn / (tn + fp)\n",
    "                    if (tp+fp) != 0:\n",
    "                        ppv = tp / (tp + fp)\n",
    "                    else:\n",
    "                        ppv = np.nan\n",
    "                    if (tn +fn) != 0:\n",
    "                        npv = tn / (tn + fn)\n",
    "                    else:\n",
    "                        npv = np.nan\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][f\"sensitivity at {threshold: .0%} threshold\"] = sensitivity\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][f\"specificity at {threshold: .0%} threshold\"] = specificity\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][f\"ppv at {threshold: .0%} threshold\"] = ppv\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][f\"npv at {threshold: .0%} threshold\"] = npv\n",
    "        \n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"auc\"] = auc\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"auc ci\"] = auc_ci\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"discrimination slope\"] = discrimination_slope\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"discrimination slope ci\"] = discrimination_slope_ci\n",
    "        \n",
    "                # calibration\n",
    "                y_true_np = y_true.astype(int)\n",
    "                y_pred_prob_np = y_pred_prob.astype(float)\n",
    "                y_pred_prob_np = np.clip(y_pred_prob.astype(float), 1e-15, 1 - 1e-15)\n",
    "                logit_preds = np.log(y_pred_prob_np / (1 - y_pred_prob_np))\n",
    "                X = sm.add_constant(logit_preds)\n",
    "\n",
    "                calibration_intercept = \"-\"\n",
    "                calibration_slope = \"-\"\n",
    "                calibration_intercept_ci = (\"-\", \"-\")\n",
    "                calibration_slope_ci = (\"-\", \"-\")\n",
    "                \n",
    "                if not len(np.unique(X)) <= 1:\n",
    "                    try:\n",
    "                        calib_model = sm.Logit(y_true_np, X).fit(disp=0)\n",
    "                        calibration_intercept = calib_model.params[0]\n",
    "                        calibration_slope = calib_model.params[1]\n",
    "                        \n",
    "                        ci = calib_model.conf_int()\n",
    "                        calibration_intercept_ci = ci[0]\n",
    "                        calibration_slope_ci = ci[1]\n",
    "                    except np.linalg.LinAlgError:\n",
    "                        pass\n",
    "                \n",
    "                observed, expected = np.sum(y_true), np.sum (y_pred_prob)\n",
    "                oe_ratio = observed / expected if expected > 0 else np.nan\n",
    "                obs_ci_lower, obs_ci_upper = (0, -np.log(0.05/2)) if observed == 0 else (stats.chi2.ppf(0.05/2, 2*observed) / 2, stats.chi2.ppf(1-0.05/2, 2*(observed+1)) / 2) # assuming Poisson distribution\n",
    "                oe_ratio_ci_lower, oe_ratio_ci_upper = (obs_ci_lower / expected, obs_ci_upper / expected) if expected > 0 else (np.nan, np.nan)\n",
    "                oe_ratio_pval = 2*min(stats.poisson.cdf(observed, expected), 1 - stats.poisson.cdf(observed - 1, expected))\n",
    "\n",
    "                sort_idx = np.argsort(y_pred_prob_np)\n",
    "                y_sorted, p_sorted = y_true_np[sort_idx], y_pred_prob_np[sort_idx]\n",
    "                lowess_result = lowess(y_sorted, p_sorted, frac=0.2, return_sorted=False)\n",
    "                calibrated_probs = np.empty_like(lowess_result)\n",
    "                calibrated_probs[sort_idx] = lowess_result\n",
    "                abs_diff = np.abs(calibrated_probs - y_pred_prob_np)\n",
    "                ici, e50, e90, emax = np.mean(abs_diff), np.median(abs_diff), np.percentile(abs_diff, 90), np.max(abs_diff)\n",
    "                \n",
    "                if all([np.count_nonzero(y_true == y) > 1 for y in np.unique(y_true)]):\n",
    "                    ce = CalibrationEvaluator(y_true, y_pred_prob, outsample=True, n_groups=\"auto\")\n",
    "                    ce_metrics_dict = ce.metrics()\n",
    "                    hltest_result = ce.hosmerlemeshow(verbose=False)\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"hosmerlemeshow_statistic\"] = hltest_result[0]\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"hosmerlemeshow_p\"] = hltest_result[1]\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"ce\"] = ce\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"ce_contingency_table\"] = ce.contingency_table\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"adaptive calibration error\"] = ce_metrics_dict[2]\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"maximum calibration error\"] = ce_metrics_dict[3]\n",
    "                else:\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"hosmerlemeshow_statistic\"] = np.nan\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"hosmerlemeshow_p\"] = np.nan\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"ce\"] = np.nan\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"ce_contingency_table\"] = np.nan\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"adaptive calibration error\"] = np.nan\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"maximum calibration error\"] = np.nan\n",
    "                bins = [0.0, 0.05, 0.15, 0.5, 0.85, 1.0]\n",
    "                guideline_thresholds = [\"<5%\", '515%', '1550%', '5085%', '>85%']\n",
    "                guideline_threshold_labels = [\"Very Low\", 'Low', 'Moderate', 'High', \"Very High\"]\n",
    "                df_preds = pd.DataFrame({'y_pred_prob': y_pred_prob, 'y_true': y_true, 'guideline_category': pd.cut(y_pred_prob, bins=bins, labels=guideline_thresholds, include_lowest=True)})\n",
    "                guideline_category_prevalence_df = df_preds.groupby('guideline_category', observed=False)['y_true'].mean() # prevalence per group\n",
    "                guideline_category_prevalence_df = guideline_category_prevalence_df.fillna(\"-\")\n",
    "                guideline_category_proportion_df = df_preds.groupby('guideline_category', observed=False).count()['y_true']/df_preds.shape[0] # proportion per group\n",
    "                guideline_category_proportion_df = guideline_category_proportion_df.fillna(0)\n",
    "        \n",
    "                score_dict[\"logit_preds\"] = logit_preds\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_intercept\"] = calibration_intercept\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_intercept_ci\"] = calibration_intercept_ci\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_slope\"] = calibration_slope\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_slope_ci\"] = calibration_slope_ci\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"expected vs. observed outcome ESC 2024 guideline categories\"] = guideline_category_prevalence_df\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"proportion per ESC 2024 guideline categories\"] = guideline_category_proportion_df\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"o:e ratio\"] = oe_ratio\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"o:e ratio 95%CI\"] = [float(oe_ratio_ci_lower), float(oe_ratio_ci_upper)]\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"o:e ratio p-value\"] = oe_ratio_pval\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"ici\"] = ici\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"e50\"] = e50\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"e90\"] = e90\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"emax\"] = emax                \n",
    "                \n",
    "                # reclassification\n",
    "                if score != \"CardioExplorer\":\n",
    "                    if \"CardioExplorer\" in scores:\n",
    "                        score_key = \"CardioExplorer\"\n",
    "                    else:\n",
    "                        score_key = \"CardioExplorer individual PTP\"\n",
    "                    df_reclassification = pd.DataFrame({\"y_true\": y_true, \"score\": y_pred_prob, \"CardioExplorer\": scores[score_key][\"ptp_values\"]})\n",
    "                    continuos_nri_df, _ = nri(df_reclassification, \"score\", \"CardioExplorer\", \"y_true\")\n",
    "                    nri_threshold_df = pd.DataFrame()\n",
    "                    for threshold in bins[1:-1]:\n",
    "                        nri_threshold_df = pd.concat([nri_threshold_df, calculate_nri_with_threshold(df_reclassification, \"score\", \"CardioExplorer\", \"y_true\", threshold)]) \n",
    "                    cat1 = pd.cut(y_pred_prob, bins=bins, labels=guideline_thresholds)\n",
    "                    cat2 = pd.cut(scores[score_key][\"ptp_values\"], bins=bins, labels=guideline_thresholds)\n",
    "                    df_cook_table = pd.DataFrame({'true': y_true, score: cat1, 'CardioExplorer': cat2})\n",
    "                    table_event = pd.crosstab(df_cook_table[df_cook_table['true'] == 1][score], df_cook_table[df_cook_table['true'] == 1]['CardioExplorer'])\n",
    "                    table_event = table_event.applymap(lambda n : f\"{n} ({round(n/table_event.sum().sum(), 2)})\")\n",
    "                    table_nonevent = pd.crosstab(df_cook_table[df_cook_table['true'] == 0][score], df_cook_table[df_cook_table['true'] == 0]['CardioExplorer'])\n",
    "                    table_nonevent = table_nonevent.applymap(lambda n : f\"{n} ({round(n/table_nonevent.sum().sum(), 2)})\")\n",
    "                    idi = evaluation_per_score_and_outcome[outcome][\"scores\"][score_key][\"discrimination slope\"] - discrimination_slope\n",
    "                    categorial_nri, categorial_nri_ci, categorial_nri_p = bootstrap_nri(y_pred_prob, scores[score_key][\"ptp_values\"], y_true, bins)\n",
    "        \n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"CardioExplorer continous nri\"] = continuos_nri_df # probably too optimistic\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"CardioExplorer Cook's reclassification table event\"] = table_event\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"CardioExplorer Cook's reclassification table nonevent\"] = table_nonevent\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"CardioExplorer Integrated Discrimination Improvement\"] = continuos_nri_df.loc[\"IDI\", :]\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"CardioExplorer categorial nri\"] = pd.DataFrame({\"NRI\": {\"index with 95 CI\": f\"{round(categorial_nri, 2)}({round(categorial_nri_ci[0], 2)}-{round(categorial_nri_ci[1], 2)})\", \"p-value\": categorial_nri_p}}).T\n",
    "                    evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"CardioExplorer threshold-based nri\"] = nri_threshold_df # probably too optimistic\n",
    "                \n",
    "                # decision curve analysis\n",
    "                dca_df = calculate_dca(y_true, y_pred_prob, model_name=score, harm=0)\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"dca_df\"] = dca_df\n",
    "                \n",
    "                dca_df_harm = calculate_dca(y_true, y_pred_prob, model_name=score, harm=harms_per_score[score])\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"dca_df_including_test_harm\"] = dca_df_harm\n",
    "                evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"test_harm\"] = harms_per_score[score]\n",
    "                \n",
    "    score_comparison_scores_and_outcomes[score_comparison_key][\"evaluation_per_score_and_outcome\"] = evaluation_per_score_and_outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751f896-3c12-44c2-b451-49164409d4ff",
   "metadata": {},
   "source": [
    "## Create folder structure for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc449b8a-9a55-4e71-898f-ffbad2692089",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"analysis\"\n",
    "folders_to_create = [\"general\", \"calibration\", \"discrimination\", \"clinical_usefulness\", \"reclassification\"]\n",
    "output_folder = Path(output_folder)\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "outpath = output_folder.joinpath(now)\n",
    "Path.mkdir(outpath)\n",
    "for folder in folders_to_create:\n",
    "    Path.mkdir(outpath.joinpath(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d488b-5d87-47c8-a877-c2174b3c6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_replace(text: str, replacements: dict) -> str:\n",
    "    pattern = re.compile(\"|\".join(map(re.escape, replacements.keys())))\n",
    "    return pattern.sub(lambda m: replacements[m.group(0)], text)\n",
    "\n",
    "short_scores = {\"CardioExplorer individual PTP\": \"CEi\", \"CardioExplorer\": \"CE\", \"PROMISE Minimal-Risk Score\": \"PMRS\", \"ESC 2024 PTP (formula)\": \"E24PTP\", \"ESC 2024 RF-CL (formula)\": \"E24RFCL\", \"ESC 2019 PTP\": \"E19PTP\", \"ESC 2013 PTP\": \"E13PTP\", \"ESC 2024 CACS-CL (formula)\": \"E24CSCL\", \"ACC 2021 PTP\": \"A21PTP\", \"ACC 2012 PTP\": \"A12PTP\", 'Diamond-Forrester': \"DF\", \"ESC 2024 RF-CL (formula with default 0 for number of risk factors and symptom score)\": \"E24RFCLd\", \"CAD consortium score 2012 Basic PTP\": \"CAD12PTP\", \"CAD consortium score 2012 Clinical PTP\": \"CAD12CL\", \"clinical assessment of CAD pretest probability\": \"clin\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6dda9-5af6-4317-aa7b-b89a15bc50d3",
   "metadata": {},
   "source": [
    "## Plots start from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03b4c0-f1d4-447a-9a71-4d971a8e4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_group_variables.to_excel(outpath.joinpath(\"general/per_group_variables.xlsx\"))\n",
    "per_group_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb774ba-bfc1-4b48-acc6-82e8237aca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "against_reference_variables.to_excel(outpath.joinpath(\"general/reference.xlsx\"))\n",
    "against_reference_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0be29-fa51-410e-b5a4-01d9aeee8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "against_reference_variables_z.to_excel(outpath.joinpath(\"general/reference_z.xlsx\"))\n",
    "against_reference_variables_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece6517-54ba-4767-99b8-dd79ce152686",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    short_key = multi_replace(score_comparison_key, short_scores)\n",
    "    short_key = short_key.replace(\" vs. \", \"-\")\n",
    "    Path.mkdir(outpath.joinpath(f\"calibration/{short_key}\"))\n",
    "    for outcome, outcome_dict in score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"].items():\n",
    "        Path.mkdir(outpath.joinpath(f\"calibration/{short_key}/{outcome}\"))\n",
    "        evaluation_per_score_and_outcome = score_comparison_scores_and_outcomes[score_comparison_key][\"evaluation_per_score_and_outcome\"]\n",
    "        y_true = outcome_dict[\"y_true\"]\n",
    "        scores = outcome_dict[\"scores\"]\n",
    "        combined_calibration_plot = {}\n",
    "        for score, score_dict in scores.items(): \n",
    "            if all([np.count_nonzero(y_true == y) > 1 for y in np.unique(y_true)]) and len(np.unique(y_true)) == 2:\n",
    "                ce = evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"ce\"]\n",
    "                # nonparametric is a calibration curve derived using the LOWESS algorithm\n",
    "                plot, x_nonparametric, y_nonparametric = ce.calibration_plot(colors={\"Perfectly calibrated\": \"black\", \"Nonparametric\": colors_hex[score], \"histogram\": \"grey\", \"Grouped observations\": \"red\"}, frac=0.2)\n",
    "                if isinstance(evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_intercept\"], str):\n",
    "                    plt.title(f\"Calibration Intercept: {evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_intercept\"]}, Calibration Slope: {evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_slope\"]}\\nn={len(y_true)}\", fontsize=10)\n",
    "                else:\n",
    "                    plt.title(f\"Calibration Intercept: {evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_intercept\"]:.4f}, Calibration Slope: {evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"calibration_slope\"]:.4f}\\nn={len(y_true)}\", fontsize=10)\n",
    "                plt.suptitle(f\"Calibration plot for {score} compared to {outcome}\", fontsize=14)\n",
    "                plt.show(plot)\n",
    "                plot.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/calibration_plot_{score}.svg\"))\n",
    "                plot.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/calibration_plot_{score}.png\"))\n",
    "                combined_calibration_plot[score] = (x_nonparametric, y_nonparametric)\n",
    "                \n",
    "                predictions = scores[score][\"ptp_values\"]\n",
    "                cases = predictions[y_true == 1]\n",
    "                non_cases = predictions[y_true == 0]\n",
    "\n",
    "                fig, ax = plt.subplots()\n",
    "                sns.histplot(non_cases, bins=30, alpha=0.6, label='non-cases',\n",
    "                    color='blue', kde=True, stat='count', ax=ax)\n",
    "                sns.histplot(-cases, bins=30, alpha=0.6, label='cases',\n",
    "                    color='yellow', kde=True, stat='count', ax=ax)\n",
    "                plt.ylim(-len(scores[score][\"ptp_values\"]), len(scores[score][\"ptp_values\"]))\n",
    "                plt.xlim(-1, 1)\n",
    "                plt.xlabel(\"Predicted probability\")\n",
    "                plt.title(f\"Distribution of predictions for {score}\")\n",
    "                plt.show()\n",
    "                fig.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/histogram_mirror_{score}.svg\"))\n",
    "                fig.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/histogram_mirror_{score}.png\"))\n",
    "\n",
    "                fig, ax = plt.subplots()\n",
    "                sns.histplot(data=scores[score][\"ptp_values\"])\n",
    "                plt.ylim(-len(scores[score][\"ptp_values\"]), len(scores[score][\"ptp_values\"]))\n",
    "                plt.xlim(0, 1)\n",
    "                plt.xlabel(\"Predicted probability\")\n",
    "                plt.title(f\"Distribution of predictions for {score}\")\n",
    "                plt.show()\n",
    "                fig.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/histogram_combined_{score}.svg\"))\n",
    "                fig.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/histogram_combined_{score}.png\"))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        for score_name, xy in combined_calibration_plot.items():\n",
    "            x, y = xy[0], xy[1]\n",
    "            ax.plot(x, y, label=score_name, color=colors_hex[score_name])\n",
    "        ax.set_xlabel('Predicted Probability')\n",
    "        ax.set_ylabel('Actual Probability')\n",
    "        ax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        plt.suptitle(f\"Combined calibration plot compared to {outcome}\", fontsize=14)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/combined_calibration_plot.svg\"))\n",
    "        fig.savefig(outpath.joinpath(f\"calibration/{short_key}/{outcome}/combined_calibration_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088bff5-ac2f-4eeb-a995-1d4b81f2064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostic accuracy (AUC plots)\n",
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    short_key = multi_replace(score_comparison_key, short_scores)\n",
    "    short_key = short_key.replace(\" vs. \", \"-\")\n",
    "    Path.mkdir(outpath.joinpath(f\"discrimination/{short_key}/\"))\n",
    "    for outcome, outcome_dict in score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"].items():\n",
    "        evaluation_per_score_and_outcome = score_comparison_scores_and_outcomes[score_comparison_key][\"evaluation_per_score_and_outcome\"]\n",
    "        if outcome in evaluation_per_score_and_outcome:\n",
    "            Path.mkdir(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/\"))\n",
    "            y_true = outcome_dict[\"y_true\"]\n",
    "            scores = outcome_dict[\"scores\"]\n",
    "            fig, ax = plt.subplots()\n",
    "            plot_chance_level = True\n",
    "            for score, score_dict in scores.items(): \n",
    "                score_values = score_dict[\"score_values\"]\n",
    "                metrics.RocCurveDisplay.from_predictions(\n",
    "                    y_true,\n",
    "                    score_values,\n",
    "                    name=score,\n",
    "                    plot_chance_level=plot_chance_level,\n",
    "                    despine=True,\n",
    "                    ax=ax,\n",
    "                    color=colors_hex[score],\n",
    "                )\n",
    "                plot_chance_level = False\n",
    "            _ = ax.set(\n",
    "                xlabel=\"False Positive Rate\",\n",
    "                ylabel=\"True Positive Rate\",\n",
    "            )\n",
    "            plt.suptitle(f\"Diagnostic accuracy compared to {outcome}\", fontsize=14)\n",
    "            plt.title(f\"n={len(outcome_dict['y_true'])}\", fontsize=10)\n",
    "            plt.show()\n",
    "            fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/comparison_AUC.svg\"))\n",
    "            fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/comparison_AUC.png\"))\n",
    "    \n",
    "            score_color_dict_density_plot = {}\n",
    "            for score, score_dict in scores.items():\n",
    "                color = colors_hex[score]\n",
    "                color_neg = adjust_color_brightness(color, 0.5)\n",
    "                color_pos = adjust_color_brightness(color, 1.5)\n",
    "                score_color_dict_density_plot[f\"{score} positive\"] = color_pos\n",
    "                score_color_dict_density_plot[f\"{score} negative\"] = color_neg\n",
    "                score_values = score_dict[\"score_values\"]\n",
    "                discrimination_slope = evaluation_per_score_and_outcome[outcome][\"scores\"][score][\"discrimination slope\"]\n",
    "                df_pred = pd.DataFrame({'y_true': y_true, 'score_values': score_values})\n",
    "                df_pred['y_true_label'] = df_pred['y_true'].map({0: 'negative', 1: 'positive'})\n",
    "                fig = plt.figure(figsize=(8, 5))\n",
    "                sns.boxplot(data=df_pred, x='y_true_label', y='score_values', palette={\"negative\": color_neg, \"positive\": color_pos})\n",
    "                plt.suptitle(f\"Discrimination box plot for {score} compared to {outcome}\", fontsize=14)\n",
    "                plt.title(f\"Discrimination Slope: {discrimination_slope:.3f}\\nn={len(outcome_dict['y_true'])}\", fontsize=10)\n",
    "                plt.ylabel(\"Predicted probability\")\n",
    "                plt.xlabel(\"Outcome\")\n",
    "                plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/{short_scores[score]}_boxplot.svg\"))\n",
    "                fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/{short_scores[score]}_boxplot.png\"))\n",
    "        \n",
    "                fig = plt.figure(figsize=(8, 5))\n",
    "                sns.kdeplot(data=df_pred, x='score_values', hue='y_true_label', fill=True, common_norm=False, palette={\"negative\": color_neg, \"positive\": color_pos}, alpha=0.15)\n",
    "                mean_neg = df_pred.loc[df_pred[\"y_true\"] == 0, \"score_values\"].mean()\n",
    "                mean_pos = df_pred.loc[df_pred[\"y_true\"] == 1, \"score_values\"].mean()\n",
    "                plt.axvline(mean_neg, color=color_neg, linestyle='--', label=f'Mean negative: {mean_neg:.2f}')\n",
    "                plt.axvline(mean_pos, color=color_pos, linestyle='--', label=f'Mean positive: {mean_pos:.2f}')\n",
    "                plt.suptitle(f\"Discrimination density plot for {score} compared to {outcome}\", fontsize=14)\n",
    "                plt.title(f\"Discrimination Slope: {discrimination_slope:.3f}\\nn={len(outcome_dict['y_true'])}\", fontsize=10)\n",
    "                plt.ylabel(\"Density\")\n",
    "                plt.xlabel(\"Score value\")\n",
    "                plt.legend()\n",
    "                plt.grid(True, linestyle='--', alpha=0.5)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/{short_scores[score]}_kdeplot.svg\"))\n",
    "                fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/{short_scores[score]}_kdeplot.png\"))\n",
    "        \n",
    "            df_preds = pd.DataFrame()\n",
    "            for score, score_dict in scores.items():\n",
    "                score_values = score_dict[\"score_values\"]\n",
    "                scaler = MinMaxScaler()\n",
    "                score_values = scaler.fit_transform(score_values.reshape(-1, 1)).flatten()\n",
    "                df_pred = pd.DataFrame({'score_values': score_values, 'y_true': y_true, 'score': score})\n",
    "                df_preds = pd.concat([df_preds, df_pred], ignore_index=True)\n",
    "            df_preds['score outcome'] = df_preds['score'] + \" \" + df_preds['y_true'].replace({0: \"negative\", 1: \"positive\"}).astype(str)\n",
    "        \n",
    "            fig = plt.figure(figsize=(10, 6))\n",
    "            sns.kdeplot(data=df_preds, x='score_values', hue='score outcome', multiple='layer', fill=True, alpha=0.3, linewidth=1.5, palette=score_color_dict_density_plot)\n",
    "            plt.suptitle(f\"Discrimination density plot compared to {outcome}\", fontsize=14)\n",
    "            plt.title(f\"n={len(outcome_dict['y_true'])}\", fontsize=10)\n",
    "            plt.xlabel(\"Score value normalised\")\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/comparison_kdeplot.svg\"))\n",
    "            fig.savefig(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/comparison_kdeplot.png\"))\n",
    "        \n",
    "            specificity_and_sensitivity_per_score_and_threshold = {}\n",
    "            for i in scores:\n",
    "                entries = []\n",
    "                for j in [\"sensitivity\", \"specificity\", \"ppv\", \"npv\"]:\n",
    "                    for k in [0.05, 0.15, 0.85]:\n",
    "                        entries.append(((j, k), evaluation_per_score_and_outcome[outcome][\"scores\"][i][f\"{j} at {k: .0%} threshold\"] ))\n",
    "                specificity_and_sensitivity_per_score_and_threshold[i] = dict(entries)\n",
    "            specificity_and_sensitivity_per_score_and_threshold_df = pd.DataFrame(specificity_and_sensitivity_per_score_and_threshold)\n",
    "            specificity_and_sensitivity_per_score_and_threshold_df.to_excel(outpath.joinpath(f\"discrimination/{short_key}/{outcome}/specificity_sensitivity.xlsx\"))\n",
    "            print(\"specificity and sensitivity per score and threshold for\", outcome)\n",
    "            display(specificity_and_sensitivity_per_score_and_threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74855c57-0761-48ad-a818-523d58c48efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reclassification\n",
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    short_key = multi_replace(score_comparison_key, short_scores)\n",
    "    short_key = short_key.replace(\" vs. \", \"-\")\n",
    "    Path.mkdir(outpath.joinpath(f\"reclassification/{short_key}/\"))\n",
    "    for outcome, outcome_dict in score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"].items():\n",
    "        evaluation_per_score_and_outcome = score_comparison_scores_and_outcomes[score_comparison_key][\"evaluation_per_score_and_outcome\"]\n",
    "        if outcome in evaluation_per_score_and_outcome:\n",
    "            Path.mkdir(outpath.joinpath(f\"reclassification/{short_key}/{outcome}/\"))\n",
    "            for score, score_dict in evaluation_per_score_and_outcome[outcome][\"scores\"].items():\n",
    "                if score != \"CardioExplorer\":\n",
    "                    print(\"Cook's reclassification table for events\")\n",
    "                    display(score_dict[\"CardioExplorer Cook's reclassification table event\"])\n",
    "                    print(\"Cook's reclassification table for non-events\")\n",
    "                    display(score_dict[\"CardioExplorer Cook's reclassification table nonevent\"])\n",
    "                    score_dict[\"CardioExplorer Cook's reclassification table event\"].to_excel(outpath.joinpath(f\"reclassification/{short_key}/{outcome}/Cook table events CE vs. {short_scores[score]}.xlsx\"))\n",
    "                    score_dict[\"CardioExplorer Cook's reclassification table nonevent\"].to_excel(outpath.joinpath(f\"reclassification/{short_key}/{outcome}/Cook table nonevents CE vs. {short_scores[score]}.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62a613-6dc6-434c-a4ea-509a72fd949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCA plot \n",
    "show_standardized = False\n",
    "# plots adapted from https://mskcc-epi-bio.github.io/decisioncurveanalysis/dca-tutorial-python.html\n",
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    short_key = multi_replace(score_comparison_key, short_scores)\n",
    "    short_key = short_key.replace(\" vs. \", \"-\")\n",
    "    Path.mkdir(outpath.joinpath(f\"clinical_usefulness/{short_key}/\"))\n",
    "    for outcome, outcome_dict in score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"].items():\n",
    "        evaluation_per_score_and_outcome = score_comparison_scores_and_outcomes[score_comparison_key][\"evaluation_per_score_and_outcome\"]\n",
    "        if outcome in evaluation_per_score_and_outcome:\n",
    "            Path.mkdir(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/\"))\n",
    "            scores = outcome_dict[\"scores\"]\n",
    "            y_true = outcome_dict[\"y_true\"]\n",
    "            prevalence = np.mean(y_true)\n",
    "            fig, ax = plt.subplots()\n",
    "            for score, score_dict in evaluation_per_score_and_outcome[outcome][\"scores\"].items():\n",
    "                plot_dca(score_dict[\"dca_df\"], ax, score_dict[\"dca_df\"][\"standardized_net_benefit\"] if show_standardized else score_dict[\"dca_df\"][\"net_benefit\"], score, smooth_fraction=0.5, color=colors_hex[score])\n",
    "            for strategy, strategy_dict in evaluation_per_score_and_outcome[outcome][\"extreme_strategies\"].items():\n",
    "                plot_dca(strategy_dict[\"dca_df\"], ax, strategy_dict[\"dca_df\"][\"standardized_net_benefit\"] if show_standardized else strategy_dict[\"dca_df\"][\"net_benefit\"], strategy, smooth_fraction=0.5, color=colors_hex[strategy])\n",
    "            plt.suptitle(f\"Decision Curve Analysis for {outcome}\", fontsize=14)\n",
    "            plt.title(f\"n={len(outcome_dict[\"y_true\"])}\", fontsize=10)\n",
    "            plt.xlabel(\"Threshold Probability\")\n",
    "            plt.ylabel(\"Standardized Net Benefit\" if show_standardized else \"Net Benefit\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.ylim(-0.05, 1.1)\n",
    "            plt.xlim(0, 0.3) # plots only the area with relevant clinical decision thresholds\n",
    "            plt.show()\n",
    "            fig.savefig(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/DCA_plot.svg\"))\n",
    "            fig.savefig(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/DCA_plot.png\"))\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            for score, score_dict in evaluation_per_score_and_outcome[outcome][\"scores\"].items():\n",
    "                plot_dca(score_dict[\"dca_df\"], ax, score_dict[\"dca_df\"][\"standardized_net_benefit_negative\"] if show_standardized else score_dict[\"dca_df\"][\"net_benefit_negative\"], score, smooth_fraction=0.5, color=colors_hex[score])\n",
    "            for strategy, strategy_dict in evaluation_per_score_and_outcome[outcome][\"extreme_strategies\"].items():\n",
    "                plot_dca(strategy_dict[\"dca_df\"], ax, strategy_dict[\"dca_df\"][\"standardized_net_benefit_negative\"] if show_standardized else strategy_dict[\"dca_df\"][\"net_benefit_negative\"], strategy, smooth_fraction=0.5, color=colors_hex[strategy])\n",
    "            plt.suptitle(f\"Decision Curve Analysis for {outcome}\", fontsize=14)\n",
    "            plt.title(f\"n = {len(outcome_dict[\"y_true\"])}\\n NB negative\", fontsize=10)\n",
    "            plt.xlabel(\"Threshold Probability\")\n",
    "            plt.ylabel(\"Standardized Net Benefit Negative\" if show_standardized else \"Net Benefit Negative\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.ylim(-0.05, 1.1)\n",
    "            plt.xlim(0, 0.3) # plots only the area with relevant clinical decision thresholds\n",
    "            plt.show()\n",
    "            fig.savefig(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/DCA_negative_plot.svg\"))\n",
    "            fig.savefig(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/DCA_negative_plot.png\"))\n",
    "            \n",
    "            fig, ax = plt.subplots()\n",
    "            for score, score_dict in evaluation_per_score_and_outcome[outcome][\"scores\"].items():\n",
    "                overall_benefit = (score_dict[\"dca_df\"][\"standardized_net_benefit\"] + score_dict[\"dca_df\"][\"standardized_net_benefit_negative\"]) if show_standardized else (score_dict[\"dca_df\"][\"net_benefit\"] + score_dict[\"dca_df\"][\"net_benefit_negative\"])\n",
    "                plot_dca(score_dict[\"dca_df\"], ax, overall_benefit, score, smooth_fraction=0.5, color=colors_hex[score])\n",
    "            for strategy, strategy_dict in evaluation_per_score_and_outcome[outcome][\"extreme_strategies\"].items():\n",
    "                overall_benefit = (strategy_dict[\"dca_df\"][\"standardized_net_benefit_negative\"] + strategy_dict[\"dca_df\"][\"standardized_net_benefit\"]) if show_standardized else (strategy_dict[\"dca_df\"][\"net_benefit_negative\"] + strategy_dict[\"dca_df\"][\"net_benefit\"])\n",
    "                plot_dca(strategy_dict[\"dca_df\"], ax, overall_benefit, strategy, smooth_fraction=0.5, color=colors_hex[strategy])\n",
    "            plt.suptitle(f\"Decision Curve Analysis for {outcome}\", fontsize=14)\n",
    "            plt.title(f\"n = {len(outcome_dict[\"y_true\"])}\\n Overall Benefit\", fontsize=10)\n",
    "            plt.xlabel(\"Threshold Probability\")\n",
    "            plt.ylabel(\"Standardized Overall Benefit\" if show_standardized else \"Overall Benefit\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.ylim(-0.05, 1.1)\n",
    "            plt.xlim(0, 0.3) # plots only the area with relevant clinical decision thresholds\n",
    "            plt.show()\n",
    "            fig.savefig(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/DCA_overall_plot.svg\"))\n",
    "            fig.savefig(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/DCA_overall_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4233962-9754-4c2a-ad3e-6e59280c25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAD_relevant_diagnostic = []\n",
    "not_CAD_relevant_diagnostic = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cce3c6-7514-4627-b5a5-d45228eaeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.get_cmap(\"tab20b\").colors\n",
    "diagnostics_colors = {\n",
    "    key: f\"rgb({int(r*255)}, {int(g*255)}, {int(b*255)})\"\n",
    "    for i, (key, (r, g, b)) in enumerate(\n",
    "        zip(CAD_relevant_diagnostic, [colors[i % len(colors)] for i in range(len(CAD_relevant_diagnostic))]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803f421-d6cc-4340-b379-f1c0b91eca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_for_pie = 0.03\n",
    "for group, pseudonyms in pseudonyms_per_group.items():\n",
    "    diagnostic_pseudonym = {}\n",
    "    multiple_diagnostics_n = 0\n",
    "    category_count = {k.lower(): 0 for k in CAD_relevant_diagnostic}\n",
    "    for i, row in df_data.iterrows():\n",
    "        pseudonym = row[\"pseudonym\"]\n",
    "        if pseudonym in pseudonyms:\n",
    "            diagnostics = row[\"performed, planned, or recommended initial diagnostics\"].split(\", \")\n",
    "            diagnostics = [d.replace(\"-\", \" \") for d in diagnostics]\n",
    "            diagnostic_pseudonym[pseudonym] = {\"diagnostic\": diagnostics}\n",
    "            for d in diagnostics:\n",
    "                matching_category = [c for c in category_count if c in d.lower()]\n",
    "                if matching_category:\n",
    "                    category_count[matching_category[0]]+=1\n",
    "                else:\n",
    "                    if d not in not_CAD_relevant_diagnostic:\n",
    "                        print(d)\n",
    "            multiple_diagnostics_n += len(diagnostics)\n",
    "    category_count_for_pie = {\"Others\": 0}\n",
    "    for c, v in category_count.items():\n",
    "        if c == \"Others\":\n",
    "            category_count_for_pie[c] += v\n",
    "        else:\n",
    "            if (v/multiple_diagnostics_n) > threshold_for_pie:\n",
    "                category_count_for_pie[c] = v\n",
    "            else:\n",
    "                category_count_for_pie[\"Others\"] += v\n",
    "    fig = go.Figure(data=[go.Pie(labels=list(category_count_for_pie.keys()), values=list(category_count_for_pie.values()), title=f\"Diagnostics for cohort {group} n={len(pseudonyms)}, n(multiple categories)={multiple_diagnostics_n}\", marker=dict(colors=[diagnostics_colors[k] for k in category_count_for_pie.keys()]), hole=0)])\n",
    "    fig.show()\n",
    "    fig.write_image(outpath.joinpath(f\"general/{group}_recommended_diagnostics_pie_chart.png\"), engine=\"orca\")\n",
    "    fig.write_image(outpath.joinpath(f\"general/{group}_recommended_diagnostics_pie_chart.svg\"), engine=\"orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ab20c-f4d0-4521-86f6-2931edbd3183",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_to_category = {\n",
    "    \"Hypertophic cardiomyopathy\": [],\n",
    "    \"Acute myocardial infarction\": [],\n",
    "    \"Heart failure\": [],\n",
    "    \"Coronary artery disease\": [],\n",
    "    \"Other chest pain\": [],\n",
    "    \"Atrial fibrillation\": [],\n",
    "    \"Dysrhythmias other than atrial fibrillation\": [], \n",
    "    \"Myocarditis\": [],\n",
    "    \"Valvular disease\": [],\n",
    "    \"Hypertension\": [],\n",
    "\n",
    "    \"Others\": [],\n",
    "    \"follow-up\": [],\n",
    "    \n",
    "    \"Pericardial effusion\": [],\n",
    "    \"Connatal heart defect\": [], \n",
    "\n",
    "    \"Aortic dissection\": [],\n",
    "    \"Aneurysm\": [],\n",
    "    \"Aortic stenosis\": [],\n",
    "    \n",
    "\n",
    "    \"Pneumonia\": [],\n",
    "    \"Pulmonary embolism\": [],\n",
    "    \"Pneumothorax\": [],\n",
    "    \"Pleuritis\": [],\n",
    "    \"Pleural effusion\": [],\n",
    "    \"COPD\": [],\n",
    "\n",
    "    \"Abdominal causes\": [],\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b7020-462e-47fd-be27-bdc61fec6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.get_cmap(\"tab20b\").colors\n",
    "diagnosis_colors = {\n",
    "    key: f\"rgb({int(r*255)}, {int(g*255)}, {int(b*255)})\"\n",
    "    for i, (key, (r, g, b)) in enumerate(\n",
    "        zip(diagnosis_to_category.keys(), [colors[i % len(colors)] for i in range(len(diagnosis_to_category))])\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab1219-c866-4bbd-8fd6-2a671a188793",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_for_pie = 0.03\n",
    "for group, pseudonyms in pseudonyms_per_group.items():\n",
    "    diagnosis_category_pseudonym = {}\n",
    "    category_count = {k: 0 for k in diagnosis_to_category.keys()}\n",
    "    multiple_categories_n = 0\n",
    "    for i, row in df_data.iterrows():\n",
    "        pseudonym = row[\"pseudonym\"]\n",
    "        if pseudonym in pseudonyms:\n",
    "            diagnosis = row[\"diagnosis at discharge or inpatient admission\"]\n",
    "            category = [c for c, examples in diagnosis_to_category.items() if any([str(e).lower() in str(diagnosis).lower() for e in examples])] \n",
    "            diagnosis_category_pseudonym[pseudonym] = {\"diagnosis\": diagnosis, \"category\": category, \"multiple_categories\": len(category)}\n",
    "            for c in category:\n",
    "                category_count[c] += 1\n",
    "            if not category:\n",
    "                print(diagnosis)\n",
    "            multiple_categories_n += len(category)\n",
    "    category_count_for_pie = {\"Others\": 0}\n",
    "    for c, v in category_count.items():\n",
    "        if c == \"Others\":\n",
    "            category_count_for_pie[c] += v\n",
    "        else:\n",
    "            if (v/multiple_categories_n) > threshold_for_pie:\n",
    "                category_count_for_pie[c] = v\n",
    "            else:\n",
    "                print(c, v)\n",
    "                category_count_for_pie[\"Others\"] += v\n",
    "    fig = go.Figure(data=[go.Pie(labels=list(category_count_for_pie.keys()), values=list(category_count_for_pie.values()), title=f\"Diagnosis categorized for cohort {group} n={len(pseudonyms)}, n(multiple categories)={multiple_categories_n}\", marker=dict(colors=[diagnosis_colors[k] for k in category_count_for_pie.keys()]), hole=0)])\n",
    "    fig.show()\n",
    "    fig.write_image(outpath.joinpath(f\"general/{group}_included_diagnoses_pie_chart.png\"), engine=\"orca\")\n",
    "    fig.write_image(outpath.joinpath(f\"general/{group}_included_diagnoses_pie_chart.svg\"), engine=\"orca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bad5d-e7d1-46a4-a037-7fe9bb4089bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_for_df = [\"brier\", \"nk_r2\", \"cs_r2\", 'sensitivity at  5% threshold', 'specificity at  5% threshold', 'sensitivity at  15% threshold', 'specificity at  15% threshold', 'sensitivity at  85% threshold', 'specificity at  85% threshold', 'ppv at  5% threshold', 'npv at  5% threshold', 'ppv at  15% threshold', 'npv at  15% threshold', 'ppv at  85% threshold', 'npv at  85% threshold', 'auc', 'discrimination slope', 'hosmerlemeshow_statistic', 'hosmerlemeshow_p', 'adaptive calibration error', 'maximum calibration error', 'calibration_intercept', \"calibration_intercept_ci\", 'calibration_slope', \"calibration_slope_ci\", \"o:e ratio\", \"o:e ratio 95%CI\", \"o:e ratio p-value\", \"ici\", \"e50\", \"e90\"]\n",
    "for score_comparison_key in score_comparison_scores_and_outcomes:\n",
    "    short_key = multi_replace(score_comparison_key, short_scores)\n",
    "    short_key = short_key.replace(\" vs. \", \"-\")\n",
    "    Path.mkdir(outpath.joinpath(f\"general/{short_key}\"))\n",
    "    for outcome, outcome_dict in score_comparison_scores_and_outcomes[score_comparison_key][\"outcomes\"].items():\n",
    "        Path.mkdir(outpath.joinpath(f\"general/{short_key}/{outcome}/\"))\n",
    "        evaluation_per_score_and_outcome = score_comparison_scores_and_outcomes[score_comparison_key][\"evaluation_per_score_and_outcome\"]\n",
    "        scores = outcome_dict[\"scores\"]\n",
    "        expected_observed_categories = pd.DataFrame()\n",
    "        for score, score_dict in scores.items(): \n",
    "            if outcome in evaluation_per_score_and_outcome:\n",
    "                s = evaluation_per_score_and_outcome[outcome][\"scores\"][score]\n",
    "                pd.Series({key: s[key] for key in keys_for_df}).to_excel(outpath.joinpath(f\"general/{short_key}/{outcome}/{score}_metrices.xlsx\"))\n",
    "                s['expected vs. observed outcome ESC 2024 guideline categories'].name = \"true prevalence in categories predicted by \" + short_scores[score]\n",
    "                expected_observed_categories = pd.concat([expected_observed_categories, s['expected vs. observed outcome ESC 2024 guideline categories']], axis=1)\n",
    "                s[\"dca_df\"].to_excel(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/{short_scores[score]}_dca.xlsx\"))\n",
    "                s['dca_df_including_test_harm'][\"test harm\"] = s['test_harm']\n",
    "                s['dca_df_including_test_harm'].to_excel(outpath.joinpath(f\"clinical_usefulness/{short_key}/{outcome}/{short_scores[score]}_dca_including_harm.xlsx\"))\n",
    "                if not isinstance(s['ce_contingency_table'], float):\n",
    "                    s['ce_contingency_table'].to_excel(outpath.joinpath(f\"calibration/{short_key}/{outcome}/{short_scores[score]}_contingency_table.xlsx\"))\n",
    "        expected_observed_categories.to_excel(outpath.joinpath(f\"general/{short_key}/{outcome}/comparison_expected vs. observed outcome.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235a787-9d90-4a8f-a329-30f1094e5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessor_list = [\"MACE 3 months (Major Adverse Cardiovascular Events)\", \"percutaneous coronary angiography performed 3 months\", \"non-invasive ischemia testing performed 3 months\", \n",
    "                     \"coronary computed tomography angiography performed 3 months\", \"stress electrocardiography performed 3 months\"]\n",
    "for score in ['CardioExplorer',\n",
    " \"CardioExplorer individual PTP\",\n",
    " 'PROMISE Minimal-Risk Score', \n",
    " 'ESC 2024 PTP (formula)', \n",
    " 'ESC 2024 RF-CL (formula)', \n",
    " 'ESC 2019 PTP',\n",
    " 'ACC 2021 PTP',\n",
    " 'Diamond-Forrester',\n",
    " 'ESC 2024 RF-CL (formula with default 0 for number of risk factors and symptom score)',\n",
    " 'CAD consortium score 2012 Basic PTP',\n",
    " 'CAD consortium score 2012 Clinical PTP']:\n",
    "    bins = [0.0, 0.05, 0.15, 0.5, 0.85, 1.0]\n",
    "    guideline_thresholds = [\"<5%\", '515%', '1550%', '5085%', '>85%']\n",
    "    score_subset = df_scores.loc[:, [(score, \"ptp_values\"), (\"pseudonym\", \" \")]]\n",
    "    score_subset = score_subset.loc[~score_subset[(score, \"ptp_values\")].isna(), :]\n",
    "    score_subset = pd.DataFrame({\n",
    "        'pseudonym': score_subset[(\"pseudonym\", \" \")], \n",
    "        f'category ({score})': pd.cut(score_subset[(score, \"ptp_values\")], bins, labels=guideline_thresholds, include_lowest=True)\n",
    "                                })\n",
    "    for assessor in assessor_list:\n",
    "        score_subset = score_subset.merge(df_data[['pseudonym', assessor]], on='pseudonym', how='left')\n",
    "    score_subset.dropna(inplace=True)\n",
    "    score_subset = score_subset.replace({\"no information\": 0, \"none\": 0, \"no\": 0})\n",
    "    score_subset[\"no further diagnosis\"] = (~(score_subset[assessor_list]!=0).any(axis=1)).astype(int)\n",
    "    score_subset[assessor_list] = (score_subset[assessor_list] != 0).astype(int)\n",
    "    full_assessor_list = assessor_list + [\"no further diagnosis\"]\n",
    "    score_subset = score_subset.groupby(f'category ({score})')[full_assessor_list].sum()\n",
    "    score_subset.to_excel(outpath.joinpath(f\"general/diagnostics_per_category_{score}.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
